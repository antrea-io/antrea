# Provide the active flow record timeout as a duration string. This determines
# how often the flow aggregator exports the active flow records to the flow
# collector. Thus, for flows with a continuous stream of packets, a flow record
# will be exported to the collector once the elapsed time since the last export
# event in the flow aggregator is equal to the value of this timeout.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
activeFlowRecordTimeout: {{ .Values.activeFlowRecordTimeout }}

# Provide the inactive flow record timeout as a duration string. This determines
# how often the flow aggregator exports the inactive flow records to the flow
# collector. A flow record is considered to be inactive if no matching record
# has been received by the flow aggregator in the specified interval.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
inactiveFlowRecordTimeout: {{ .Values.inactiveFlowRecordTimeout }}

# Provide the transport protocol for the flow aggregator collecting process, which is tls, tcp or udp.
aggregatorTransportProtocol: {{ .Values.aggregatorTransportProtocol | quote }}

# Provide an extra DNS name or IP address of flow aggregator for generating TLS certificate.
flowAggregatorAddress: {{ .Values.flowAggregatorAddress | quote }}

# recordContents enables configuring some fields in the flow records. Fields can
# be excluded to reduce record size, but some features or external tooling may
# depend on these fields.
recordContents:
  # Determine whether source and destination Pod labels will be included in the flow records.
  podLabels: {{ .Values.recordContents.podLabels }}

# apiServer contains APIServer related configuration options.
apiServer:
  # The port for the flow-aggregator APIServer to serve on.
  apiPort: {{ .Values.apiServer.apiPort }}

  # Comma-separated list of Cipher Suites. If omitted, the default Go Cipher Suites will be used.
  # https://golang.org/pkg/crypto/tls/#pkg-constants
  # Note that TLS1.3 Cipher Suites cannot be added to the list. But the apiserver will always
  # prefer TLS1.3 Cipher Suites whenever possible.
  tlsCipherSuites: {{ .Values.apiServer.tlsCipherSuites | quote }}

  # TLS min version from: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13.
  tlsMinVersion: {{ .Values.apiServer.tlsMinVersion | quote }}

# flowCollector contains external IPFIX or JSON collector related configuration options.
flowCollector:
  # Enable is the switch to enable exporting flow records to external flow collector.
  enable: {{ .Values.flowCollector.enable }}

  # Provide the flow collector address as string with format <IP>:<port>[:<proto>], where proto is tcp or udp.
  # If no L4 transport proto is given, we consider tcp as default.
  address: {{ .Values.flowCollector.address | quote }}

  # Provide the 32-bit Observation Domain ID which will uniquely identify this instance of the flow
  # aggregator to an external flow collector. If omitted, an Observation Domain ID will be generated
  # from the persistent cluster UUID generated by Antrea. Failing that (e.g. because the cluster UUID
  # is not available), a value will be randomly generated, which may vary across restarts of the flow
  # aggregator.
  {{- if .Values.flowCollector.observationDomainID }}
  observationDomainID: {{ .Values.flowCollector.observationDomainID }}
  {{- else }}
  #observationDomainID:
  {{- end }}

  # Provide format for records sent to the configured flow collector.
  # Supported formats are IPFIX and JSON.
  recordFormat: {{ .Values.flowCollector.recordFormat | quote }}

# clickHouse contains ClickHouse related configuration options.
clickHouse:
  # Enable is the switch to enable exporting flow records to ClickHouse.
  enable: {{ .Values.clickHouse.enable }}

  # Database is the name of database where Antrea "flows" table is created.
  database: "default"

  # DatabaseURL is the url to the database. Provide the database URL as a string with format
  # <Protocol>://<ClickHouse server FQDN or IP>:<ClickHouse port>. The protocol has to be
  # one of the following: "tcp", "tls", "http", "https". When "tls" or "https" is used, tls
  # will be enabled.
  databaseURL: {{ .Values.clickHouse.databaseURL | quote }}

  # TLS configuration options, when using TLS to connect to the ClickHouse service.
  tls:
    # InsecureSkipVerify determines whether to skip the verification of the server's certificate chain and host name.
    # Default is false.
    insecureSkipVerify: {{ .Values.clickHouse.tls.insecureSkipVerify }}

    # CACert indicates whether to use custom CA certificate. Default root CAs will be used if this field is false.
    # If true, a Secret named "clickhouse-ca" must be provided with the following keys:
    # ca.crt: <CA certificate>
    caCert: {{ .Values.clickHouse.tls.caCert }}

  # Debug enables debug logs from ClickHouse sql driver.
  debug: {{ .Values.clickHouse.debug }}

  # Compress enables lz4 compression when committing flow records.
  compress: {{ .Values.clickHouse.compress }}

  # CommitInterval is the periodical interval between batch commit of flow records to DB.
  # Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
  # The minimum interval is 1s based on ClickHouse documentation for best performance.
  commitInterval: {{ .Values.clickHouse.commitInterval | quote }}

# s3Uploader contains configuration options for uploading flow records to AWS S3.
s3Uploader:
  # Enable is the switch to enable exporting flow records to AWS S3.
  # At the moment, the flow aggregator will look for the "standard" environment variables to
  # authenticate to AWS. These can be static credentials (AWS_ACCESS_KEY_ID,
  # AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN) or a Web Identity Token
  # (AWS_WEB_IDENTITY_TOKEN_FILE).
  enable: {{ .Values.s3Uploader.enable }}

  # BucketName is the name of the S3 bucket to which flow records will be uploaded. If this
  # field is empty, initialization will fail.
  bucketName: {{ .Values.s3Uploader.bucketName | quote }}

  # BucketPrefix is the prefix ("folder") under which flow records will be uploaded. If this
  # is omitted, flow records will be uploaded to the root of the bucket.
  bucketPrefix: {{ .Values.s3Uploader.bucketPrefix | quote }}

  # Region is used as a "hint" to get the region in which the provided bucket is located.
  # An error will occur if the bucket does not exist in the AWS partition the region hint
  # belongs to. If region is omitted, the value of the AWS_REGION environment variable will
  # be used, and if it is missing, we will default to "us-west-2".
  region: {{ .Values.s3Uploader.region | quote }}

  # RecordFormat defines the format of the flow records uploaded to S3. Only "CSV" is
  # supported at the moment.
  recordFormat: {{ .Values.s3Uploader.recordFormat | quote }}

  # Compress enables gzip compression when uploading files to S3. Defaults to true.
  compress: {{ .Values.s3Uploader.compress }}

  # MaxRecordsPerFile is the maximum number of records per file uploaded. It is not recommended
  # to change this value.
  maxRecordsPerFile: {{ .Values.s3Uploader.maxRecordsPerFile }}

  # UploadInterval is the duration between each file upload to S3.
  uploadInterval: {{ .Values.s3Uploader.uploadInterval | quote }}

# FlowLogger contains configuration options for writing flow records to a local log file.
flowLogger:
  # Enable is the switch to enable writing flow records to a local log file.
  enable: {{ .Values.flowLogger.enable }}

  # Path is the path to the local log file.
  path: {{ .Values.flowLogger.path | quote }}

  # MaxSize is the maximum size in MB of a log file before it gets rotated.
  maxSize: {{ .Values.flowLogger.maxSize }}

  # MaxBackups is the maximum number of old log files to retain. If set to 0, all log files will be
  # retained (unless MaxAge causes them to be deleted).
  maxBackups: {{ .Values.flowLogger.maxBackups }}

  # MaxAge is the maximum number of days to retain old log files based on the timestamp encoded in
  # their filename. The default (0) is not to remove old log files based on age.
  maxAge: {{ .Values.flowLogger.maxAge }}

  # Compress enables gzip compression on rotated files.
  compress: {{ .Values.flowLogger.compress }}

  # RecordFormat defines the format of the flow records logged to file. Only "CSV" is supported at
  # the moment.
  recordFormat: {{ .Values.flowLogger.recordFormat | quote }}

  # Filters can be used to select which flow records to log to file. The provided filters are OR-ed
  # to determine whether a specific flow should be logged.
  filters:
    {{- toYaml .Values.flowLogger.filters | trim | nindent 6 }}

  # PrettyPrint enables conversion of some numeric fields to a more meaningful string
  # representation.
  prettyPrint: {{ .Values.flowLogger.prettyPrint }}
