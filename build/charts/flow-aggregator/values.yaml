# -- Container image used by Flow Aggregator.
image: 
  repository: "projects.registry.vmware.com/antrea/flow-aggregator"
  pullPolicy: "IfNotPresent"
  tag: ""

# -- Provide the active flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
activeFlowRecordTimeout: 60s
# -- Provide the inactive flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
inactiveFlowRecordTimeout: 90s
# -- Provide the transport protocol for the flow aggregator collecting process, which is tls, tcp or udp.
aggregatorTransportProtocol: "tls"
# -- Provide DNS name or IP address of flow aggregator for generating TLS certificate. It must match
# the flowCollectorAddr parameter in the antrea-agent config.
flowAggregatorAddress: "flow-aggregator.flow-aggregator.svc"
# recordContents enables configuring some fields in the flow records.
recordContents:
  # -- Determine whether source and destination Pod labels will be included in the flow records.
  podLabels: false
# apiServer contains APIServer related configuration options.
apiServer:
  # -- The port for the Flow Aggregator APIServer to serve on.
  apiPort: 10348
  # -- Comma-separated list of cipher suites that will be used by the Flow Aggregator
  # APIservers. If empty, the default Go Cipher Suites will be used.
  tlsCipherSuites: ""
  # -- TLS min version from: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13.
  tlsMinVersion: ""
# flowCollector contains external IPFIX or JSON collector related configuration options.
flowCollector:
  # -- Determine whether to enable exporting flow records to external flow collector.
  enable: false
  # -- Provide the flow collector address as string with format <IP>:<port>[:<proto>], 
  # where proto is tcp or udp. If no L4 transport proto is given, we consider tcp as default.
  address: ""
  # -- Provide the 32-bit Observation Domain ID which will uniquely identify this instance of the flow
  # aggregator to an external flow collector. If omitted, an Observation Domain ID will be generated
  # from the persistent cluster UUID generated by Antrea.
  observationDomainID: ""
  # -- Provide format for records sent to the configured flow collector.
  # Supported formats are IPFIX and JSON.
  recordFormat: "IPFIX"
# clickHouse contains ClickHouse related configuration options.
clickHouse:
  # -- Determine whether to enable exporting flow records to ClickHouse.
  enable: false
  # -- DatabaseURL is the url to the database. TCP protocol is required.
  databaseURL: "tcp://clickhouse-clickhouse.flow-visibility.svc:9000"
  # -- Debug enables debug logs from ClickHouse sql driver.
  debug: false
  # -- Compress enables lz4 compression when committing flow records.
  compress: true
  # -- CommitInterval is the periodical interval between batch commit of flow records to DB.
  # Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
  commitInterval: "8s"
  # -- Credentials to connect to ClickHouse. They will be stored in a Secret.
  connectionSecret:
    username : "clickhouse_operator"
    password: "clickhouse_operator_password"
# s3Uploader contains configuration options for uploading flow records to AWS S3.
s3Uploader:
  # -- Determine whether to enable exporting flow records to AWS S3.
  enable: false
  # -- BucketName is the name of the S3 bucket to which flow records will be uploaded. It is required.
  bucketName: ""
  # -- BucketPrefix is the prefix ("folder") under which flow records will be uploaded.
  bucketPrefix: ""
  # -- Region is used as a "hint" to get the region in which the provided bucket is located.
  # An error will occur if the bucket does not exist in the AWS partition the region hint belongs to.
  region: "us-west-2"
  # -- RecordFormat defines the format of the flow records uploaded to S3. Only "CSV" is supported at the moment.
  recordFormat: "CSV"
  # -- Compress enables gzip compression when uploading files to S3.
  compress: true
  # -- MaxRecordsPerFile is the maximum number of records per file uploaded. It is not recommended
  # to change this value.
  maxRecordsPerFile: 1000000
  # -- UploadInterval is the duration between each file upload to S3.
  uploadInterval: "60s"
  # -- Credentials to authenticate to AWS. They will be stored in a Secret and injected into the Pod
  # as environment variables.
  awsCredentials:
    aws_access_key_id: "changeme"
    aws_secret_access_key: "changeme"
    aws_session_token: ""
testing:
  ## -- Enable code coverage measurement (used when testing Flow Aggregator only).
  coverage: false
## -- Log verbosity switch for Flow Aggregator.
logVerbosity: 0
