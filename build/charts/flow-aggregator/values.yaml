# -- Container image used by Flow Aggregator.
image: 
  repository: "antrea/flow-aggregator"
  pullPolicy: "IfNotPresent"
  tag: ""

# -- Prority class to use for the flow-aggregator Pod.
priorityClassName: "system-cluster-critical"
# -- Run the flow-aggregator Pod in the host network. With hostNetwork enabled, it is usually
# necessary to set dnsPolicy to ClusterFirstWithHostNet.
hostNetwork: false
# -- DNS Policy for the flow-aggregator Pod. If empty, the Kubernetes default will be used.
dnsPolicy: ""

# Configuration specific to the flow-aggregator container.
flowAggregator:
  # -- Configure the security context for the flow-aggregator container.
  securityContext: {}
  # -- Resource requests and limits for the flow-aggregator container.
  resources:
    requests:
      cpu: "500m"
      memory: "256Mi"

# -- Mode in which to run the flow aggregator. Must be one of "Aggregate" or "Proxy". In Aggregate
# mode, flow records received from source and destination are aggregated and sent as one flow
# record. In Proxy mode, flow records are enhanced with some additional information, then sent
# directly without buffering or aggregation.
mode: "Aggregate"
# -- Provide the active flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
activeFlowRecordTimeout: 60s
# -- Provide the inactive flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
inactiveFlowRecordTimeout: 90s
# -- Provide the transport protocol for the flow aggregator collecting process, which is tls, tcp or udp.
aggregatorTransportProtocol: "tls"
# -- Provide an extra DNS name or IP address of flow aggregator for generating TLS certificate.
flowAggregatorAddress: ""
# -- Provide a clusterID to be added to records. This is only consumed by the
# flowCollector (IPFIX) exporter.
clusterID: ""
# recordContents enables configuring some fields in the flow records.
recordContents:
  # -- Determine whether source and destination Pod labels will be included in the flow records.
  podLabels: false
# -- HostAliases to be injected into the Pod's hosts file.
# For example: `[{"ip": "8.8.8.8", "hostnames": ["clickhouse.example.com"]}]`
hostAliases: []
# apiServer contains APIServer related configuration options.
apiServer:
  # -- The port for the Flow Aggregator APIServer to serve on.
  apiPort: 10348
  # -- Comma-separated list of cipher suites that will be used by the Flow Aggregator
  # APIservers. If empty, the default Go Cipher Suites will be used.
  tlsCipherSuites: ""
  # -- TLS min version from: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13.
  tlsMinVersion: ""
# flowCollector contains external IPFIX or JSON collector related configuration options.
flowCollector:
  # -- Determine whether to enable exporting flow records to external flow collector.
  enable: false
  # -- Provide the flow collector address as string with format <IP>:<port>[:<proto>], 
  # where proto is tcp or udp. If no L4 transport proto is given, we consider tcp as default.
  address: ""
  # -- Provide the 32-bit Observation Domain ID which will uniquely identify this instance of the flow
  # aggregator to an external flow collector. If omitted, an Observation Domain ID will be generated
  # from the persistent cluster UUID generated by Antrea.
  observationDomainID: ""
  # -- Provide format for records sent to the configured flow collector.
  # Supported formats are IPFIX and JSON.
  recordFormat: "IPFIX"
  # -- Template retransmission interval when using the udp protocol to export records.
  # The value must be provided as a duration string. Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
  templateRefreshTimeout: "600s"
  # -- Maximum message size to use for IPFIX records. If set to 0 (recommended), a reasonable
  # default value will be used based on the protocol (tcp or udp) used to connect to the collector.
  # Min valid value is 512 and max valid value is 65535.
  maxIPFIXMsgSize: 0
  # TLS / mTLS configuration when exporting to the flowCollector.
  # This is only available when using tcp as the protocol.
  tls:
    # -- Enable TLS.
    enable: false
    # -- Name of the Secret containing the CA certificate used to authenticate the
    # flowCollector. Default root CAs will be used if this field is empty. The Secret must be
    # created in the Namespace in which the Flow Aggregator is deployed, and it must contain the
    # ca.crt key.
    caSecretName: ""
    # -- ServerName is used to verify the hostname on the returned certificates. It is also included
    # in the client's handshake (SNI) to support virtual hosting unless it is an IP address. If this
    # field is omitted, the hostname used for certificate verification will default to the provided
    # server address (flowCollector.address).
    serverName: ""
    # -- Name of the Secret containing the client's certificate and private key for mTLS. If
    # omitted, client authentication will be disabled. The Secret must be created in Namespace in
    # which the Flow Aggregator is deployed, and it must be of type kubernetes.io/tls and contain
    # the tls.crt and tls.key keys.
    clientSecretName: ""
    # -- Minimum TLS version from: VersionTLS12, VersionTLS13.
    # @default -- VersionTLS12
    minVersion: ""

# clickHouse contains ClickHouse related configuration options.
clickHouse:
  # -- Determine whether to enable exporting flow records to ClickHouse.
  enable: false
  # -- DatabaseURL is the url to the database. Provide the database URL as a string with format
  # <Protocol>://<ClickHouse server FQDN or IP>:<ClickHouse port>. The protocol has to be one of
  # the following: "tcp", "tls", "http", "https". When "tls" or "https" is used, tls will be enabled.
  databaseURL: "tcp://clickhouse-clickhouse.flow-visibility.svc:9000"
  # TLS configuration options, when using TLS to connect to the ClickHouse service.
  tls:
    # -- Determine whether to skip the verification of the server's certificate chain and host name. Default is false.
    insecureSkipVerify: false
    # -- Indicates whether to use custom CA certificate. Default root CAs will be used if this field is false.
    # If true, a Secret named "clickhouse-ca" must be provided with the following keys:
    # ca.crt: <CA certificate>
    caCert: false
  # -- Debug enables debug logs from ClickHouse sql driver.
  debug: false
  # -- Compress enables lz4 compression when committing flow records.
  compress: true
  # -- CommitInterval is the periodical interval between batch commit of flow records to DB.
  # Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
  commitInterval: "8s"
  # -- Credentials to connect to ClickHouse. They will be stored in a Secret.
  connectionSecret:
    username : "clickhouse_operator"
    password: "clickhouse_operator_password"
# s3Uploader contains configuration options for uploading flow records to AWS S3.
s3Uploader:
  # -- Determine whether to enable exporting flow records to AWS S3.
  enable: false
  # -- BucketName is the name of the S3 bucket to which flow records will be uploaded. It is required.
  bucketName: ""
  # -- BucketPrefix is the prefix ("folder") under which flow records will be uploaded.
  bucketPrefix: ""
  # -- Region is used as a "hint" to get the region in which the provided bucket is located.
  # An error will occur if the bucket does not exist in the AWS partition the region hint belongs to.
  region: "us-west-2"
  # -- RecordFormat defines the format of the flow records uploaded to S3. Only "CSV" is supported at the moment.
  recordFormat: "CSV"
  # -- Compress enables gzip compression when uploading files to S3.
  compress: true
  # -- MaxRecordsPerFile is the maximum number of records per file uploaded. It is not recommended
  # to change this value.
  maxRecordsPerFile: 1000000
  # -- UploadInterval is the duration between each file upload to S3.
  uploadInterval: "60s"
  # -- Credentials to authenticate to AWS. They will be stored in a Secret and injected into the Pod
  # as environment variables.
  awsCredentials:
    aws_access_key_id: "changeme"
    aws_secret_access_key: "changeme"
    aws_session_token: ""
# flowLogger contains configuration options for writing flow records to a local log file.
flowLogger:
  # -- Determine whether to enable exporting flow records to a local log file.
  enable: false
  # -- Path is the path to the local log file.
  path: "/tmp/antrea-flows.log"
  # -- MaxSize is the maximum size in MB of a log file before it gets rotated.
  maxSize: 100
  # -- MaxBackups is the maximum number of old log files to retain. If set to 0, all log files will
  # be retained (unless MaxAge causes them to be deleted).
  maxBackups: 3
  # -- MaxAge is the maximum number of days to retain old log files based on the timestamp encoded
  # in their filename. The default (0) is not to remove old log files based on age.
  maxAge: 0
  # -- Compress enables gzip compression on rotated files.
  compress: true
  # -- RecordFormat defines the format of the flow records logged to file. Only "CSV" is supported at the moment.
  recordFormat: "CSV"
  # -- Filters can be used to select which flow records to log to file. The provided filters are
  # OR-ed to determine whether a specific flow should be logged. By default, all flows are logged.
  # With the following filters, only flows which are denied because of a network policy will be logged:
  # [{ingressNetworkPolicyRuleActions: ["Drop", "Reject"]}, {egressNetworkPolicyRuleActions: ["Drop", "Reject"]}]
  filters: []
  # -- PrettyPrint enables conversion of some numeric fields to a more meaningful string representation.
  prettyPrint: true
testing:
  # -- Enable code coverage measurement (used when testing Flow Aggregator only).
  coverage: false
# -- Log verbosity switch for Flow Aggregator.
logVerbosity: 0
# -- Namespace in which Antrea was installed.
antreaNamespace: "kube-system"
