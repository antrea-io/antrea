# -- Container image used by Flow Aggregator.
image: 
  repository: "antrea/flow-aggregator"
  pullPolicy: "IfNotPresent"
  tag: ""

# -- Provide the active flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
activeFlowRecordTimeout: 60s
# -- Provide the inactive flow record timeout as a duration string.
# Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
inactiveFlowRecordTimeout: 90s
# -- Provide the transport protocol for the flow aggregator collecting process, which is tls, tcp or udp.
aggregatorTransportProtocol: "tls"
# -- Provide an extra DNS name or IP address of flow aggregator for generating TLS certificate.
flowAggregatorAddress: ""
# recordContents enables configuring some fields in the flow records.
recordContents:
  # -- Determine whether source and destination Pod labels will be included in the flow records.
  podLabels: false
# -- HostAliases to be injected into the Pod's hosts file.
# For example: `[{"ip": "8.8.8.8", "hostnames": ["clickhouse.example.com"]}]`
hostAliases: []
# apiServer contains APIServer related configuration options.
apiServer:
  # -- The port for the Flow Aggregator APIServer to serve on.
  apiPort: 10348
  # -- Comma-separated list of cipher suites that will be used by the Flow Aggregator
  # APIservers. If empty, the default Go Cipher Suites will be used.
  tlsCipherSuites: ""
  # -- TLS min version from: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13.
  tlsMinVersion: ""
# flowCollector contains external IPFIX or JSON collector related configuration options.
flowCollector:
  # -- Determine whether to enable exporting flow records to external flow collector.
  enable: false
  # -- Provide the flow collector address as string with format <IP>:<port>[:<proto>], 
  # where proto is tcp or udp. If no L4 transport proto is given, we consider tcp as default.
  address: ""
  # -- Provide the 32-bit Observation Domain ID which will uniquely identify this instance of the flow
  # aggregator to an external flow collector. If omitted, an Observation Domain ID will be generated
  # from the persistent cluster UUID generated by Antrea.
  observationDomainID: ""
  # -- Provide format for records sent to the configured flow collector.
  # Supported formats are IPFIX and JSON.
  recordFormat: "IPFIX"
# clickHouse contains ClickHouse related configuration options.
clickHouse:
  # -- Determine whether to enable exporting flow records to ClickHouse.
  enable: false
  # -- DatabaseURL is the url to the database. Provide the database URL as a string with format
  # <Protocol>://<ClickHouse server FQDN or IP>:<ClickHouse port>. The protocol has to be one of
  # the following: "tcp", "tls", "http", "https". When "tls" or "https" is used, tls will be enabled.
  databaseURL: "tcp://clickhouse-clickhouse.flow-visibility.svc:9000"
  # TLS configuration options, when using TLS to connect to the ClickHouse service.
  tls:
    # -- Determine whether to skip the verification of the server's certificate chain and host name. Default is false.
    insecureSkipVerify: false
    # -- Indicates whether to use custom CA certificate. Default root CAs will be used if this field is false.
    # If true, a Secret named "clickhouse-ca" must be provided with the following keys:
    # ca.crt: <CA certificate>
    caCert: false
  # -- Debug enables debug logs from ClickHouse sql driver.
  debug: false
  # -- Compress enables lz4 compression when committing flow records.
  compress: true
  # -- CommitInterval is the periodical interval between batch commit of flow records to DB.
  # Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
  commitInterval: "8s"
  # -- Credentials to connect to ClickHouse. They will be stored in a Secret.
  connectionSecret:
    username : "clickhouse_operator"
    password: "clickhouse_operator_password"
# s3Uploader contains configuration options for uploading flow records to AWS S3.
s3Uploader:
  # -- Determine whether to enable exporting flow records to AWS S3.
  enable: false
  # -- BucketName is the name of the S3 bucket to which flow records will be uploaded. It is required.
  bucketName: ""
  # -- BucketPrefix is the prefix ("folder") under which flow records will be uploaded.
  bucketPrefix: ""
  # -- Region is used as a "hint" to get the region in which the provided bucket is located.
  # An error will occur if the bucket does not exist in the AWS partition the region hint belongs to.
  region: "us-west-2"
  # -- RecordFormat defines the format of the flow records uploaded to S3. Only "CSV" is supported at the moment.
  recordFormat: "CSV"
  # -- Compress enables gzip compression when uploading files to S3.
  compress: true
  # -- MaxRecordsPerFile is the maximum number of records per file uploaded. It is not recommended
  # to change this value.
  maxRecordsPerFile: 1000000
  # -- UploadInterval is the duration between each file upload to S3.
  uploadInterval: "60s"
  # -- Credentials to authenticate to AWS. They will be stored in a Secret and injected into the Pod
  # as environment variables.
  awsCredentials:
    aws_access_key_id: "changeme"
    aws_secret_access_key: "changeme"
    aws_session_token: ""
# flowLogger contains configuration options for writing flow records to a local log file.
flowLogger:
  # -- Determine whether to enable exporting flow records to a local log file.
  enable: false
  # -- Path is the path to the local log file.
  path: "/tmp/antrea-flows.log"
  # -- MaxSize is the maximum size in MB of a log file before it gets rotated.
  maxSize: 100
  # -- MaxBackups is the maximum number of old log files to retain. If set to 0, all log files will
  # be retained (unless MaxAge causes them to be deleted).
  maxBackups: 3
  # -- MaxAge is the maximum number of days to retain old log files based on the timestamp encoded
  # in their filename. The default (0) is not to remove old log files based on age.
  maxAge: 0
  # -- Compress enables gzip compression on rotated files.
  compress: true
  # -- RecordFormat defines the format of the flow records logged to file. Only "CSV" is supported at the moment.
  recordFormat: "CSV"
  # -- Filters can be used to select which flow records to log to file. The provided filters are
  # OR-ed to determine whether a specific flow should be logged. By default, all flows are logged.
  # With the following filters, only flows which are denied because of a network policy will be logged:
  # [{ingressNetworkPolicyRuleActions: ["Drop", "Reject"]}, {egressNetworkPolicyRuleActions: ["Drop", "Reject"]}]
  filters: []
  # -- PrettyPrint enables conversion of some numeric fields to a more meaningful string representation.
  prettyPrint: true
testing:
  # -- Enable code coverage measurement (used when testing Flow Aggregator only).
  coverage: false
# -- Log verbosity switch for Flow Aggregator.
logVerbosity: 0
